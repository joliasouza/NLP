# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nK2kG61ktKY9AdUC2nmD5Z3FS9GVUBRE

# Pré-processamento de texto

## Limpar texto

### Remover símbolos e pontuação
"""

pontuacao = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
texto = "Oi, tudo bem? Eu estou bem, e você?"

texto_sem_pontuacao = ""
for char in texto:
  if char not in pontuacao:
    texto_sem_pontuacao = texto_sem_pontuacao + char

print(texto_sem_pontuacao)

palavrao = "C@%@&*$ P%#@"
palavrao_sem_pontuacao = ""
for char in palavrao:
  if char not in pontuacao:
    palavrao_sem_pontuacao = palavrao_sem_pontuacao + char
print(palavrao_sem_pontuacao)

import re
texto_sem_pontuacao2 = re.sub(r'[^\w\s]', '', texto)
print(texto_sem_pontuacao2)

"""#### re.sub(padrão, substituto, string)

#### `texto_sem_pontuacao2 = re.sub(r'[^\w\s]', '', texto)`

> r'': indica que a string é raw (crua) -> caracteres de escape são tratados literalmente. útil em expressões regulares para evitar confusão com barras invertidas (\).

> [^\w\s]: o conteúdo dentro dos colchetes [] define um conjunto de caracteres a serem buscados.
*   \w: qualquer caractere [a-zA-Z0-9_]
*   \s: qualquer espaço em branco (espaços, tabulações e quebras de linha)
*    ^: negação -> qualquer caractere que não seja uma letra, número, sublinhado ou espaço em branco

> '': o que irá substituir pontos/símbolos


> texto: string sobre a qual a substituição será realizada

### Tokenization
"""

#usando python split
texto = "Oi, tudo bem? Eu estou bem, e você?"
tokens = texto.split(' ')
print(tokens)

import nltk
nltk.download('punkt')
nltk.download('wordnet')

tokens = nltk.word_tokenize(texto)
print(tokens)

"""### Remove Stop Words"""

from nltk.corpus import stopwords
nltk.download('stopwords')

stop_words = stopwords.words('portuguese')
print(stop_words)

texto_sem_stop_words = ""

for texto_sem_pontuacao2 in texto:
    if texto_sem_pontuacao2.lower() not in stop_words:
        texto_sem_stop_words += texto_sem_pontuacao2 + " "

print(texto_sem_stop_words)

"""### Stemming


"""

#import nltk
#from nltk.tokenize import word_tokenize
from nltk.stem import RSLPStemmer #para português (PorterStemmer p/ inglês)
#nltk.download('punkt')
nltk.download('rslp')

stemmer = RSLPStemmer()
texto_tokenizado = "Estudos sobre processamento de linguagem natural."
texto_tokenizado = nltk.word_tokenize(texto_tokenizado)

for word in texto_tokenizado:
  print(stemmer.stem(word))

"""### Lemmatization"""

#!python -m spacy download pt_core_news_sm
import spacy
nlp = spacy.load('pt_core_news_sm')

doc = nlp("Estudos sobre processamento de linguagem natural.")

for token in doc:
  print(token.lemma_)
